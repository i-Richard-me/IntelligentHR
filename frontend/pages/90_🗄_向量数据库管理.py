import io
import os
import sys
import json
from typing import List, Dict, Tuple

import streamlit as st
import pandas as pd
from pymilvus import (
    connections,
    Collection,
    FieldSchema,
    CollectionSchema,
    DataType,
    utility,
)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.append(project_root)

from utils.llm_tools import CustomEmbeddings
from frontend.ui_components import show_sidebar, show_footer, apply_common_styles

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½HRåŠ©æ‰‹ - Milvusæ•°æ®åº“ç®¡ç†", page_icon="ğŸ—„ï¸", layout="wide"
)

# åº”ç”¨è‡ªå®šä¹‰æ ·å¼
apply_common_styles()

# æ˜¾ç¤ºä¾§è¾¹æ 
show_sidebar()

# Milvusè¿æ¥é…ç½®
MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
MILVUS_DB = "examples"

# åŠ è½½é…ç½®æ–‡ä»¶
with open("data/config/collections_config.json", "r", encoding="utf-8") as f:
    CONFIG = json.load(f)


def connect_to_milvus():
    """è¿æ¥åˆ°Milvusæ•°æ®åº“"""
    connections.connect(
        alias="default", host=MILVUS_HOST, port=MILVUS_PORT, db_name=MILVUS_DB
    )


def create_milvus_collection(collection_config: Dict, dim: int):
    """åˆ›å»ºMilvusé›†åˆ"""
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    ]
    for field in collection_config["fields"]:
        fields.append(
            FieldSchema(name=field["name"], dtype=DataType.VARCHAR, max_length=65535)
        )
    fields.append(FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim))

    schema = CollectionSchema(fields, collection_config["description"])
    collection = Collection(collection_config["name"], schema)
    return collection


def insert_examples_to_milvus(examples: List[Dict], collection_config: Dict):
    """å°†ç¤ºä¾‹æ’å…¥åˆ°Milvusæ•°æ®åº“"""
    connect_to_milvus()

    embeddings = CustomEmbeddings(
        api_key=os.getenv("EMBEDDING_API_KEY"),
        api_base=os.getenv("EMBEDDING_API_BASE"),
        model=os.getenv("EMBEDDING_MODEL"),
    )

    data = {field["name"]: [] for field in collection_config["fields"]}
    vectors = []

    for example in examples:
        for field in collection_config["fields"]:
            data[field["name"]].append(example[field["name"]])
        vector = embeddings.embed_query(example[collection_config["embedding_field"]])
        vectors.append(vector)

    if not utility.has_collection(collection_config["name"]):
        collection = create_milvus_collection(collection_config, len(vectors[0]))
    else:
        collection = Collection(collection_config["name"])

    insert_data = list(data.values()) + [vectors]
    collection.insert(insert_data)

    index_params = {
        "metric_type": "L2",
        "index_type": "IVF_FLAT",
        "params": {"nlist": 1024},
    }
    collection.create_index("embedding", index_params)
    collection.load()

    connections.disconnect("default")
    return len(examples)


def process_csv_file(file, collection_config: Dict):
    """å¤„ç†ä¸Šä¼ çš„CSVæ–‡ä»¶"""
    examples = []
    csv_file = io.StringIO(file.getvalue().decode("utf-8"))
    df = pd.read_csv(csv_file)

    required_columns = [field["name"] for field in collection_config["fields"]]

    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„åˆ—
    missing_columns = set(required_columns) - set(df.columns)
    if missing_columns:
        raise ValueError(f"CSVæ–‡ä»¶ç¼ºå°‘ä»¥ä¸‹åˆ—: {', '.join(missing_columns)}")

    for _, row in df.iterrows():
        example = {col: row[col] for col in required_columns}
        examples.append(example)

    return examples


def get_collection_stats(collection_name: str) -> Dict:
    """è·å–é›†åˆçš„ç»Ÿè®¡ä¿¡æ¯"""
    connect_to_milvus()
    collection = Collection(collection_name)
    collection.load()

    stats = {
        "å®ä½“æ•°é‡": collection.num_entities,
        "å­—æ®µæ•°é‡": len(collection.schema.fields) - 1,  # å‡å»è‡ªåŠ¨ç”Ÿæˆçš„ id å­—æ®µ
        "ç´¢å¼•ç±»å‹": collection.index().params.get("index_type", "æœªçŸ¥"),
    }

    connections.disconnect("default")
    return stats


def display_db_management_info():
    st.info(
        """
    **ğŸ—„ï¸ Milvusæ•°æ®åº“ç®¡ç†**

    Milvusæ•°æ®åº“ç®¡ç†å·¥å…·ç”¨äºé«˜æ•ˆç®¡ç†å’Œæ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„ç¤ºä¾‹æ•°æ®ã€‚
    å®ƒæ”¯æŒCSVæ–‡ä»¶ä¸Šä¼ ã€æ•°æ®é¢„è§ˆå’Œæ‰¹é‡æ’å…¥åŠŸèƒ½ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•å‘é‡æ•°æ®é›†ã€‚
    é€šè¿‡è¿™ä¸ªå·¥å…·ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†ç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºå¹¶å­˜å‚¨åœ¨Milvusä¸­ï¼Œ
    ä¸ºåç»­çš„ç›¸ä¼¼åº¦æœç´¢å’Œæ™ºèƒ½åŒ¹é…æä¾›åŸºç¡€ã€‚
    """
    )


def display_workflow():
    with st.expander("ğŸ“‹ æŸ¥çœ‹Milvusæ•°æ®åº“ç®¡ç†å·¥ä½œæµç¨‹", expanded=False):
        st.markdown(
            """
        **1. é€‰æ‹©Collection**
        ä»é…ç½®ä¸­é€‰æ‹©è¦æ“ä½œçš„æ•°æ®é›†åˆã€‚

        **2. ä¸Šä¼ CSVæ–‡ä»¶**
        ä¸Šä¼ åŒ…å«ç¤ºä¾‹æ•°æ®çš„CSVæ–‡ä»¶ã€‚

        **3. æ•°æ®é¢„è§ˆå’Œå»é‡**
        é¢„è§ˆä¸Šä¼ çš„æ•°æ®ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®ï¼Œå¹¶è¿›è¡Œå»é‡å¤„ç†ã€‚

        **4. å‘é‡åŒ–å¤„ç†**
        å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚

        **5. æ•°æ®æ’å…¥**
        å°†å¤„ç†åçš„æ•°æ®æ’å…¥Milvusæ•°æ®åº“ã€‚

        **6. ç´¢å¼•åˆ›å»º**
        ä¸ºæ’å…¥çš„æ•°æ®åˆ›å»ºç´¢å¼•ï¼Œä¼˜åŒ–æ£€ç´¢æ€§èƒ½ã€‚
        """
        )


def get_existing_records(collection_config: Dict) -> pd.DataFrame:
    """è·å–å·²å­˜åœ¨çš„è®°å½•"""
    connect_to_milvus()
    collection = Collection(collection_config["name"])
    collection.load()

    # è·å–æ‰€æœ‰å­—æ®µå
    field_names = [field["name"] for field in collection_config["fields"]]

    # æŸ¥è¯¢æ‰€æœ‰è®°å½•
    results = collection.query(expr="id >= 0", output_fields=field_names)

    connections.disconnect("default")

    return pd.DataFrame(results)


def dedup_examples(
    new_examples: List[Dict], existing_records: pd.DataFrame, collection_config: Dict
) -> Tuple[List[Dict], List[Dict]]:
    """å¯¹æ–°ä¸Šä¼ çš„æ•°æ®è¿›è¡Œå»é‡"""
    new_df = pd.DataFrame(new_examples)

    # é€‰æ‹©ç”¨äºæ¯”è¾ƒçš„å­—æ®µï¼ˆé™¤äº†embeddingï¼‰
    compare_fields = [
        field["name"]
        for field in collection_config["fields"]
        if field["name"] != "embedding"
    ]

    # ä½¿ç”¨è¿™äº›å­—æ®µè¿›è¡Œåˆå¹¶
    merged = pd.merge(
        new_df, existing_records, on=compare_fields, how="left", indicator=True
    )

    # æ‰¾å‡ºé‡å¤å’Œæ–°å¢çš„è®°å½•
    duplicates = merged[merged["_merge"] == "both"]
    new_records = merged[merged["_merge"] == "left_only"]

    # è½¬æ¢å›å­—å…¸åˆ—è¡¨
    duplicate_examples = duplicates[compare_fields].to_dict("records")
    new_examples = new_records[compare_fields].to_dict("records")

    return new_examples, duplicate_examples


def main():
    st.title("ğŸ—„ï¸ Milvusæ•°æ®åº“ç®¡ç†")
    st.markdown("---")

    # æ˜¾ç¤ºåŠŸèƒ½ä»‹ç»
    display_db_management_info()
    st.markdown("---")

    # æ˜¾ç¤ºå·¥ä½œæµç¨‹
    display_workflow()
    st.markdown("---")

    # Collection é€‰æ‹©
    st.header("é€‰æ‹© Collection")
    with st.container(border=True):
        collection_names = list(CONFIG["collections"].keys())
        selected_collection = st.selectbox("é€‰æ‹©è¦æ“ä½œçš„Collection", collection_names)
        collection_config = CONFIG["collections"][selected_collection]

        # æ˜¾ç¤ºCollectionä¿¡æ¯å’Œç»Ÿè®¡
        st.subheader("Collectionä¿¡æ¯")
        st.write(f"åç§°: {collection_config['name']}")
        st.write(f"æè¿°: {collection_config['description']}")
        st.write("å­—æ®µ:")
        for field in collection_config["fields"]:
            st.write(f"- {field['name']}: {field['description']}")

        st.subheader("æ•°æ®ç»Ÿè®¡")
        connect_to_milvus()
        if utility.has_collection(collection_config["name"]):
            stats = get_collection_stats(collection_config["name"])
            st.write(f"å®ä½“æ•°é‡: {stats['å®ä½“æ•°é‡']}")
            st.write(f"å­—æ®µæ•°é‡: {stats['å­—æ®µæ•°é‡']}")
            st.write(f"ç´¢å¼•ç±»å‹: {stats['ç´¢å¼•ç±»å‹']}")
        else:
            st.info("è¯¥Collectionå°šæœªåˆ›å»º")
        connections.disconnect("default")

    st.header("ä¸Šä¼ å’Œæ’å…¥æ•°æ®")
    with st.container(border=True):
        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"])

        if uploaded_file is not None:
            try:
                examples = process_csv_file(uploaded_file, collection_config)
                st.success(f"æˆåŠŸè¯»å– {len(examples)} æ¡è®°å½•")

                # è·å–å·²å­˜åœ¨çš„è®°å½•
                existing_records = get_existing_records(collection_config)

                # å»é‡
                new_examples, duplicate_examples = dedup_examples(
                    examples, existing_records, collection_config
                )

                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                st.subheader("æ•°æ®é¢„è§ˆ")
                st.write(f"æ–°å¢è®°å½•: {len(new_examples)}æ¡")
                st.write(f"é‡å¤è®°å½•: {len(duplicate_examples)}æ¡")

                new_df = pd.DataFrame(new_examples[:10])  # åªæ˜¾ç¤ºå‰5æ¡æ–°è®°å½•
                st.dataframe(new_df)

                if st.button("æ’å…¥åˆ°Milvusæ•°æ®åº“"):
                    if len(new_examples) > 0:
                        with st.spinner("æ­£åœ¨æ’å…¥æ•°æ®..."):
                            inserted_count = insert_examples_to_milvus(
                                new_examples, collection_config
                            )
                        st.success(f"æˆåŠŸæ’å…¥ {inserted_count} æ¡æ–°è®°å½•åˆ°Milvusæ•°æ®åº“")
                    else:
                        st.info("æ²¡æœ‰æ–°çš„è®°å½•éœ€è¦æ’å…¥")

            except ValueError as ve:
                st.error(f"CSVæ–‡ä»¶æ ¼å¼é”™è¯¯: {str(ve)}")
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                st.error("è¯·ç¡®ä¿CSVæ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼Œå¹¶ä¸”åŒ…å«æ‰€æœ‰å¿…éœ€çš„åˆ—ã€‚")

    show_footer()


if __name__ == "__main__":
    main()
