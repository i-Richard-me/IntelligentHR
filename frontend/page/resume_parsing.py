import streamlit as st
import sys
import os
import pdfplumber
import io
import json
import requests
from bs4 import BeautifulSoup
from PIL import Image
import uuid
import asyncio
import pandas as pd
import aiohttp
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.append(project_root)

from frontend.ui_components import show_sidebar, show_footer, apply_common_styles
from backend.resume_management.extractor.resume_extraction_core import (
    process_resume,
    calculate_resume_hash,
    store_resume,
)
from backend.resume_management.storage.resume_sql_storage import get_full_resume

st.query_params.role = st.session_state.role

# è®¾ç½®æœ€å¤§å¹¶å‘æ•°
MAX_CONCURRENT_TASKS = 1

# åº”ç”¨è‡ªå®šä¹‰æ ·å¼
apply_common_styles()

show_sidebar()


def clean_html(html_content):
    """æ¸…ç†HTMLå†…å®¹ï¼Œç§»é™¤è„šæœ¬å’Œæ ·å¼"""
    soup = BeautifulSoup(html_content, "html.parser")
    for script in soup(["script", "style"]):
        script.decompose()
    return str(soup)


def extract_text_from_pdf(pdf_file):
    """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    text = ""
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text


async def extract_text_from_url(url: str, session: aiohttp.ClientSession) -> str:
    """ä»URLä¸­å¼‚æ­¥æå–æ–‡æœ¬"""
    jina_url = f"https://r.jina.ai/{url}"
    async with session.get(jina_url, ssl=False) as response:
        if response.status == 200:
            return await response.text()
        else:
            raise Exception(f"æ— æ³•ä»URLæå–å†…å®¹: {url}")


async def extract_resume_info(
    file_content, resume_id, file_type, session_id, file_or_url
):
    """æå–ç®€å†ä¿¡æ¯"""
    if file_type == "html":
        content = clean_html(file_content)
    elif file_type == "pdf":
        content = extract_text_from_pdf(io.BytesIO(file_content))
    elif file_type == "url":
        content = file_content
    else:
        st.error("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
        return None

    return await process_resume(content, resume_id, session_id, file_type, file_or_url)


def display_resume_info(resume_data):
    """æ˜¾ç¤ºæå–çš„ç®€å†ä¿¡æ¯"""
    if not resume_data:
        return

    st.markdown("## æå–çš„ç®€å†ä¿¡æ¯")

    with st.container(border=True):
        # ç®€å†æ¦‚è¿°
        display_resume_summary(resume_data)

        # ä¸ªäººä¿¡æ¯
        display_personal_info(resume_data.get("personal_info", {}))

        # æ•™è‚²èƒŒæ™¯
        display_education(resume_data.get("education", []))

        # å·¥ä½œç»å†
        display_work_experience(resume_data.get("work_experiences", []))

        # é¡¹ç›®ç»å†
        display_project_experience(resume_data.get("project_experiences", []))


def display_resume_summary(resume_data):
    """æ˜¾ç¤ºç®€å†æ¦‚è¿°"""
    with st.container(border=True):
        st.markdown("#### ç®€å†æ¦‚è¿°")
        st.markdown(f"**ç‰¹ç‚¹**: {resume_data.get('characteristics', '')}")
        st.markdown(f"**ç»éªŒ**: {resume_data.get('experience_summary', '')}")
        st.markdown(f"**æŠ€èƒ½æ¦‚è§ˆ**: {resume_data.get('skills_overview', '')}")


def display_personal_info(personal_info):
    """æ˜¾ç¤ºä¸ªäººä¿¡æ¯"""
    with st.container(border=True):
        st.markdown("#### ä¸ªäººä¿¡æ¯")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f"**å§“å:** {personal_info.get('name', 'N/A')}")
            st.markdown(f"**é‚®ç®±:** {personal_info.get('email', 'N/A')}")
        with col2:
            st.markdown(f"**ç”µè¯:** {personal_info.get('phone', 'N/A')}")
            st.markdown(f"**åœ°å€:** {personal_info.get('address', 'N/A')}")
        st.markdown(f"**ä¸ªäººç®€ä»‹:** {personal_info.get('summary', 'N/A')}")
        if personal_info.get("skills"):
            st.markdown("**æŠ€èƒ½:**")
            st.markdown(", ".join(personal_info["skills"]))


def display_education(education_list):
    """æ˜¾ç¤ºæ•™è‚²èƒŒæ™¯"""
    with st.container(border=True):
        st.markdown("#### æ•™è‚²èƒŒæ™¯")
        for edu in education_list:
            st.markdown(f"**{edu['institution']}** - {edu['degree']} in {edu['major']}")
            st.markdown(f"æ¯•ä¸šå¹´ä»½: {edu['graduation_year']}")
            st.markdown("---")


def display_work_experience(work_experiences):
    """æ˜¾ç¤ºå·¥ä½œç»å†"""
    with st.container(border=True):
        st.markdown("#### å·¥ä½œç»å†")
        for work in work_experiences:
            st.markdown(
                f"**{work['company']}** - {work['position']} ({work['experience_type']})"
            )
            st.markdown(f"{work['start_date']} to {work['end_date']}")
            st.markdown("èŒè´£:")
            for resp in work["responsibilities"]:
                st.markdown(f"- {resp}")
            st.markdown("---")


def display_project_experience(project_experiences):
    """æ˜¾ç¤ºé¡¹ç›®ç»å†"""
    if project_experiences:
        with st.container(border=True):
            st.markdown("#### é¡¹ç›®ç»å†")
            for proj in project_experiences:
                st.markdown(f"**{proj['name']}** - {proj['role']}")
                st.markdown(f"{proj['start_date']} to {proj['end_date']}")
                st.markdown("è¯¦æƒ…:")
                for detail in proj["details"]:
                    st.markdown(f"- {detail}")
                st.markdown("---")


def display_info_message():
    """æ˜¾ç¤ºæ™ºèƒ½ç®€å†è§£æç³»ç»Ÿçš„åŠŸèƒ½ä»‹ç»"""
    st.info(
        """
    æ™ºèƒ½ç®€å†è§£æç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°å¯¹å¤šç§æ ¼å¼ç®€å†çš„é«˜æ•ˆè§£æã€‚
    
    ç³»ç»Ÿèƒ½è‡ªåŠ¨æå–å’Œç»“æ„åŒ–å…³é”®ä¿¡æ¯ï¼Œæœ‰æ•ˆå¤„ç†éæ ‡å‡†åŒ–è¡¨è¿°ï¼Œæé«˜è§£æå‡†ç¡®ç‡ã€‚ä¹Ÿä¸ºåç»­çš„ç®€å†æ¨èå’Œäººæ‰ç”»åƒç­‰åº”ç”¨æä¾›äº†æ›´å¯é çš„æ•°æ®åŸºç¡€ã€‚
    """
    )


def display_workflow():
    """æ˜¾ç¤ºæ™ºèƒ½ç®€å†è§£æç³»ç»Ÿçš„å·¥ä½œæµç¨‹"""
    with st.expander("ğŸ“„ æŸ¥çœ‹æ™ºèƒ½ç®€å†è§£æå·¥ä½œæµç¨‹", expanded=False):
        col1, col2 = st.columns([1, 1])
        with col2:
            st.markdown(
                """
                <div class="workflow-container">
                    <div class="workflow-step">
                        <strong>1. æ–‡ä»¶å¤„ç†ä¸å†…å®¹æå–</strong>
                        - æ”¯æŒHTMLå’ŒPDFæ ¼å¼çš„ç®€å†æ–‡ä»¶
                    </div>
                    <div class="workflow-step">
                        <strong>2. ä¿¡æ¯è§£æä¸ç»“æ„åŒ–</strong>
                        - åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è§£æç®€å†å†…å®¹
                        - æå–ä¸ªäººä¿¡æ¯ã€æ•™è‚²èƒŒæ™¯ã€å·¥ä½œç»å†ç­‰å…³é”®ä¿¡æ¯
                    </div>
                    <div class="workflow-step">
                        <strong>3. ç®€å†æ¦‚è¿°ç”Ÿæˆ</strong>
                        - åŸºäºæå–çš„ä¿¡æ¯è‡ªåŠ¨ç”Ÿæˆç®€å†æ¦‚è¿°
                        - åŒ…æ‹¬å‘˜å·¥ç‰¹ç‚¹ã€å·¥ä½œå’Œé¡¹ç›®ç»å†ã€æŠ€èƒ½æ¦‚è§ˆç­‰
                    </div>
                    <div class="workflow-step">
                        <strong>4. ç»“æœå±•ç¤º</strong>
                        - ä»¥ç”¨æˆ·å‹å¥½çš„æ–¹å¼å¯è§†åŒ–å±•ç¤ºè§£æç»“æœ
                    </div>
                    <div class="workflow-step">
                        <strong>5. æ•°æ®å­˜å‚¨ï¼ˆå¯é€‰ï¼‰</strong>
                        - å°†è§£æåçš„æ•°æ®å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
                        - ä¸ºåç»­çš„æ£€ç´¢å’Œåˆ†ææä¾›åŸºç¡€
                    </div>
                </div>
                """,
                unsafe_allow_html=True,
            )


async def process_single_resume(
    url: str,
    session_id: str,
    semaphore: asyncio.Semaphore,
    session: aiohttp.ClientSession,
) -> None:
    """å¤„ç†å•ä¸ªç®€å†URL"""
    async with semaphore:  # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        try:
            resume_content = await extract_text_from_url(url, session)
            resume_id = calculate_resume_hash(resume_content)
            existing_resume = get_full_resume(resume_id)

            if existing_resume:
                st.warning(f"URL {url} çš„ç®€å†å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†ã€‚")
            else:
                resume_data = await process_resume(
                    resume_content, resume_id, session_id, "url", url
                )
                resume_data["resume_format"] = "url"
                resume_data["file_or_url"] = url
                store_resume(resume_data)
                st.success(f"æˆåŠŸå¤„ç† URL: {url}")
        except Exception as e:
            st.error(f"å¤„ç† URL {url} æ—¶å‡ºé”™: {str(e)}")


async def process_batch_resumes(urls: List[str], session_id: str) -> None:
    """å¼‚æ­¥å¤„ç†æ‰¹é‡ç®€å†ï¼Œæ§åˆ¶å¹¶å‘æ•°"""
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
    async with aiohttp.ClientSession() as session:
        tasks = [
            process_single_resume(url, session_id, semaphore, session) for url in urls
        ]
        await asyncio.gather(*tasks)


def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å« Streamlit åº”ç”¨çš„ä¸»è¦é€»è¾‘"""
    # åˆå§‹åŒ– session_state
    if "resume_data" not in st.session_state:
        st.session_state.resume_data = None
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    if "is_from_database" not in st.session_state:
        st.session_state.is_from_database = False

    st.title("ğŸ“„ æ™ºèƒ½ç®€å†è§£æ")
    st.markdown("---")

    display_info_message()
    display_workflow()

    st.markdown("## ç®€å†æå–")

    tab1, tab2 = st.tabs(["å•ä»½ç®€å†", "æ‰¹é‡è§£æ"])

    with tab1:
        handle_single_resume()

    with tab2:
        handle_batch_resumes()

    if st.session_state.resume_data is not None:
        display_resume_results()

    # é¡µè„š
    show_footer()


def handle_single_resume():
    """å¤„ç†å•ä»½ç®€å†ä¸Šä¼ å’ŒURLè¾“å…¥"""
    with st.container(border=True):
        uploaded_file = st.file_uploader("ä¸Šä¼ ç®€å†æ–‡ä»¶", type=["html", "pdf"])
        url_input = st.text_input("æˆ–è¾“å…¥ç®€å†URL")

        if uploaded_file is not None:
            process_uploaded_file(uploaded_file)
        elif url_input:
            asyncio.run(process_url_input(url_input))


async def process_url_input(url_input: str):
    """å¤„ç†è¾“å…¥çš„URL"""
    async with aiohttp.ClientSession() as session:
        file_content = await extract_text_from_url(url_input, session)
        resume_id = calculate_resume_hash(file_content)
        await handle_resume_processing(resume_id, "url", file_content, url_input)


def process_uploaded_file(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
    file_type = uploaded_file.type.split("/")[-1]
    file_content = uploaded_file.read()
    resume_id = calculate_resume_hash(file_content.decode("utf-8", errors="ignore"))

    handle_resume_processing(resume_id, file_type, file_content, uploaded_file.name)


async def handle_resume_processing(resume_id, file_type, file_content, file_or_url):
    """å¤„ç†ç®€å†æå–å’Œå­˜å‚¨é€»è¾‘"""
    existing_resume = get_full_resume(resume_id)
    if existing_resume:
        st.warning("æ£€æµ‹åˆ°é‡å¤çš„ç®€å†ã€‚æ­£åœ¨ä»æ•°æ®åº“ä¸­è·å–å·²è§£æçš„ä¿¡æ¯ã€‚")
        st.session_state.resume_data = existing_resume
        st.session_state.is_from_database = True
    else:
        st.session_state.is_from_database = False
        if st.button("æå–ä¿¡æ¯", key=file_type):
            with st.spinner("æ­£åœ¨æå–ç®€å†ä¿¡æ¯..."):
                resume_data = await extract_resume_info(
                    file_content,
                    resume_id,
                    file_type,
                    st.session_state.session_id,
                    file_or_url,
                )
                resume_data["resume_format"] = file_type
                resume_data["file_or_url"] = file_or_url
                st.session_state.resume_data = resume_data


def handle_batch_resumes():
    """å¤„ç†æ‰¹é‡ç®€å†ä¸Šä¼ """
    with st.container(border=True):
        batch_file = st.file_uploader("ä¸Šä¼ åŒ…å«URLçš„è¡¨æ ¼æ–‡ä»¶", type=["csv", "xlsx"])
        if batch_file is not None:
            if st.button("å¼€å§‹æ‰¹é‡å¤„ç†"):
                df = (
                    pd.read_csv(batch_file)
                    if batch_file.name.endswith(".csv")
                    else pd.read_excel(batch_file)
                )
                urls = df["URL"].tolist()

                progress_bar = st.progress(0)
                status_text = st.empty()

                async def process_with_progress():
                    total = len(urls)
                    completed = 0
                    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
                    async with aiohttp.ClientSession() as session:
                        tasks = [
                            process_single_resume(
                                url, st.session_state.session_id, semaphore, session
                            )
                            for url in urls
                        ]
                        for task in asyncio.as_completed(tasks):
                            await task
                            completed += 1
                            progress = completed / total
                            progress_bar.progress(progress)
                            status_text.text(f"å·²å¤„ç† {completed}/{total} ä¸ªURL")

                asyncio.run(process_with_progress())

                status_text.text("æ‰¹é‡å¤„ç†å®Œæˆ!")
                st.success(f"æˆåŠŸå¤„ç†äº† {len(urls)} ä¸ªURLçš„ç®€å†ã€‚")


def display_resume_results():
    """æ˜¾ç¤ºç®€å†è§£æç»“æœå’Œç›¸å…³æ“ä½œ"""
    st.markdown("---")

    display_resume_info(st.session_state.resume_data)

    # æä¾›ä¸‹è½½é€‰é¡¹
    json_string = json.dumps(st.session_state.resume_data, ensure_ascii=False, indent=2)
    st.download_button(
        label="ä¸‹è½½JSONç»“æœ",
        data=json_string,
        file_name="resume_extracted_info.json",
        mime="application/json",
    )

    # åªæœ‰å½“ç®€å†ä¸æ˜¯ä»æ•°æ®åº“ä¸­æ£€ç´¢çš„æ—¶å€™ï¼Œæ‰æ˜¾ç¤º"å­˜å‚¨ç®€å†åˆ°æ•°æ®åº“"æŒ‰é’®
    if not st.session_state.is_from_database:
        if st.button("å­˜å‚¨ç®€å†åˆ°æ•°æ®åº“"):
            with st.spinner("æ­£åœ¨å­˜å‚¨ç®€å†æ•°æ®..."):
                if store_resume(st.session_state.resume_data):
                    st.success("ç®€å†æ•°æ®å·²æˆåŠŸå­˜å‚¨åˆ°æ•°æ®åº“")
                else:
                    st.error("å­˜å‚¨ç®€å†æ•°æ®æ—¶å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•")


main()
